{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explanations for Nexora Healthcare ML Platform\n",
    "\n",
    "This notebook demonstrates various model explanation techniques for the Nexora healthcare ML platform. We'll explore how to interpret machine learning models used for clinical predictions, focusing on transparency, fairness, and clinical relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Explanation libraries\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, plot_partial_dependence\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import project modules\n",
    "from src.model_factory.fairness_metrics import FairnessEvaluator\n",
    "from src.model_factory.model_calibration import ModelCalibrator\n",
    "from src.utils.healthcare_metrics import HealthcareMetrics\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate synthetic clinical data\n",
    "# In a real environment, this would load actual patient data\n",
    "# For this notebook, we'll create synthetic data similar to the clinical_eda notebook\n",
    "\n",
    "# Function to generate synthetic patient data\n",
    "def generate_synthetic_patient_data(n_patients=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate patient IDs\n",
    "    patient_ids = [f'P{i:06d}' for i in range(n_patients)]\n",
    "    \n",
    "    # Generate demographics\n",
    "    ages = np.random.normal(65, 15, n_patients).astype(int)\n",
    "    ages = np.clip(ages, 18, 100)  # Clip to reasonable age range\n",
    "    \n",
    "    genders = np.random.choice(['M', 'F'], size=n_patients, p=[0.48, 0.52])\n",
    "    \n",
    "    races = np.random.choice(\n",
    "        ['White', 'Black', 'Hispanic', 'Asian', 'Other'],\n",
    "        size=n_patients,\n",
    "        p=[0.65, 0.13, 0.12, 0.06, 0.04]\n",
    "    )\n",
    "    \n",
    "    # Generate admission dates (within last 2 years)\n",
    "    today = datetime.now()\n",
    "    admission_days_ago = np.random.randint(1, 730, n_patients)  # Up to 2 years ago\n",
    "    admission_dates = [today - timedelta(days=days) for days in admission_days_ago]\n",
    "    \n",
    "    # Generate length of stay (1-30 days, with most stays being shorter)\n",
    "    length_of_stay = np.random.exponential(scale=5, size=n_patients).astype(int) + 1\n",
    "    length_of_stay = np.clip(length_of_stay, 1, 30)\n",
    "    \n",
    "    # Calculate discharge dates\n",
    "    discharge_dates = [admission_dates[i] + timedelta(days=length_of_stay[i]) for i in range(n_patients)]\n",
    "    \n",
    "    # Generate insurance types\n",
    "    insurance_types = np.random.choice(\n",
    "        ['Medicare', 'Medicaid', 'Private', 'Self-Pay', 'Other'],\n",
    "        size=n_patients,\n",
    "        p=[0.45, 0.15, 0.30, 0.05, 0.05]\n",
    "    )\n",
    "    \n",
    "    # Generate primary diagnoses (ICD-10 codes)\n",
    "    # Common conditions in elderly population\n",
    "    primary_diagnoses = np.random.choice(\n",
    "        ['I25.10', 'I10', 'E11.9', 'J44.9', 'I50.9', 'M17.9', 'G20', 'F03', 'N18.9', 'C50.919'],\n",
    "        size=n_patients,\n",
    "        p=[0.20, 0.18, 0.15, 0.12, 0.10, 0.08, 0.06, 0.05, 0.04, 0.02]\n",
    "    )\n",
    "    \n",
    "    # Map diagnoses to descriptions\n",
    "    diagnosis_map = {\n",
    "        'I25.10': 'Coronary artery disease',\n",
    "        'I10': 'Essential hypertension',\n",
    "        'E11.9': 'Type 2 diabetes mellitus',\n",
    "        'J44.9': 'COPD',\n",
    "        'I50.9': 'Heart failure',\n",
    "        'M17.9': 'Osteoarthritis of knee',\n",
    "        'G20': 'Parkinson\\'s disease',\n",
    "        'F03': 'Dementia',\n",
    "        'N18.9': 'Chronic kidney disease',\n",
    "        'C50.919': 'Breast cancer'\n",
    "    }\n",
    "    \n",
    "    diagnosis_descriptions = [diagnosis_map[code] for code in primary_diagnoses]\n",
    "    \n",
    "    # Generate comorbidity count (0-5)\n",
    "    comorbidity_count = np.random.poisson(lam=2, size=n_patients)\n",
    "    comorbidity_count = np.clip(comorbidity_count, 0, 5)\n",
    "    \n",
    "    # Generate medication count (0-10)\n",
    "    medication_count = comorbidity_count + np.random.randint(0, 3, n_patients)\n",
    "    medication_count = np.clip(medication_count, 0, 10)\n",
    "    \n",
    "    # Generate lab values (e.g., hemoglobin A1c for diabetes patients)\n",
    "    hba1c_values = np.where(\n",
    "        primary_diagnoses == 'E11.9',  # Diabetes patients\n",
    "        np.random.normal(8.0, 1.5, n_patients),  # Higher values for diabetics\n",
    "        np.random.normal(5.5, 0.5, n_patients)   # Normal values for non-diabetics\n",
    "    )\n",
    "    hba1c_values = np.clip(hba1c_values, 4.0, 14.0)  # Clip to reasonable range\n",
    "    \n",
    "    # Generate systolic blood pressure\n",
    "    systolic_bp = np.where(\n",
    "        primary_diagnoses == 'I10',  # Hypertension patients\n",
    "        np.random.normal(150, 15, n_patients),  # Higher values for hypertensives\n",
    "        np.random.normal(125, 10, n_patients)   # Normal values for non-hypertensives\n",
    "    )\n",
    "    systolic_bp = np.clip(systolic_bp, 90, 200)  # Clip to reasonable range\n",
    "    \n",
    "    # Generate diastolic blood pressure\n",
    "    diastolic_bp = systolic_bp * 0.6 + np.random.normal(10, 5, n_patients)\n",
    "    diastolic_bp = np.clip(diastolic_bp, 50, 120)  # Clip to reasonable range\n",
    "    \n",
    "    # Generate additional lab values\n",
    "    # Creatinine (kidney function)\n",
    "    creatinine = np.where(\n",
    "        primary_diagnoses == 'N18.9',  # Kidney disease patients\n",
    "        np.random.normal(2.5, 1.0, n_patients),  # Higher values for kidney disease\n",
    "        np.random.normal(1.0, 0.3, n_patients)   # Normal values for others\n",
    "    )\n",
    "    creatinine = np.clip(creatinine, 0.5, 8.0)  # Clip to reasonable range\n",
    "    \n",
    "    # Hemoglobin (anemia)\n",
    "    hemoglobin = np.where(\n",
    "        (primary_diagnoses == 'C50.919') | (primary_diagnoses == 'N18.9'),  # Cancer or kidney disease\n",
    "        np.random.normal(10.0, 1.5, n_patients),  # Lower values for these conditions\n",
    "        np.random.normal(13.5, 1.0, n_patients)   # Normal values for others\n",
    "    )\n",
    "    hemoglobin = np.clip(hemoglobin, 6.0, 18.0)  # Clip to reasonable range\n",
    "    \n",
    "    # Ejection fraction (heart function)\n",
    "    ejection_fraction = np.where(\n",
    "        primary_diagnoses == 'I50.9',  # Heart failure patients\n",
    "        np.random.normal(35, 10, n_patients),  # Lower values for heart failure\n",
    "        np.random.normal(60, 5, n_patients)    # Normal values for others\n",
    "    )\n",
    "    ejection_fraction = np.clip(ejection_fraction, 10, 75)  # Clip to reasonable range\n",
    "    \n",
    "    # Generate outcomes\n",
    "    # 30-day readmission (higher for certain conditions and older patients)\n",
    "    readmission_base_prob = 0.15\n",
    "    readmission_prob = readmission_base_prob + \\\n",
    "                       0.01 * (ages > 75).astype(int) + \\\n",
    "                       0.02 * (primary_diagnoses == 'I50.9').astype(int) + \\\n",
    "                       0.02 * (primary_diagnoses == 'J44.9').astype(int) + \\\n",
    "                       0.01 * (comorbidity_count >= 3).astype(int) + \\\n",
    "                       0.02 * (length_of_stay > 10).astype(int) + \\\n",
    "                       0.02 * (ejection_fraction < 40).astype(int) + \\\n",
    "                       0.01 * (creatinine > 2.0).astype(int) + \\\n",
    "                       0.01 * (hemoglobin < 10).astype(int)\n",
    "    \n",
    "    readmission_30d = np.random.binomial(1, readmission_prob, n_patients)\n",
    "    \n",
    "    # Mortality (higher for certain conditions, older patients, and longer stays)\n",
    "    mortality_base_prob = 0.05\n",
    "    mortality_prob = mortality_base_prob + \\\n",
    "                     0.02 * (ages > 80).astype(int) + \\\n",
    "                     0.03 * (primary_diagnoses == 'I50.9').astype(int) + \\\n",
    "                     0.03 * (primary_diagnoses == 'C50.919').astype(int) + \\\n",
    "                     0.01 * (length_of_stay > 14).astype(int) + \\\n",
    "                     0.02 * (comorbidity_count >= 4).astype(int) + \\\n",
    "                     0.03 * (ejection_fraction < 30).astype(int) + \\\n",
    "                     0.02 * (creatinine > 3.0).astype(int) + \\\n",
    "                     0.02 * (hemoglobin < 8).astype(int)\n",
    "    \n",
    "    mortality = np.random.binomial(1, mortality_prob, n_patients)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'patient_id': patient_ids,\n",
    "        'age': ages,\n",
    "        'gender': genders,\n",
    "        'race': races,\n",
    "        'admission_date': admission_dates,\n",
    "        'discharge_date': discharge_dates,\n",
    "        'length_of_stay': length_of_stay,\n",
    "        'insurance_type': insurance_types,\n",
    "        'primary_diagnosis_code': primary_diagnoses,\n",
    "        'primary_diagnosis': diagnosis_descriptions,\n",
    "        'comorbidity_count': comorbidity_count,\n",
    "        'medication_count': medication_count,\n",
    "        'hba1c': hba1c_values,\n",
    "        'systolic_bp': systolic_bp,\n",
    "        'diastolic_bp': diastolic_bp,\n",
    "        'creatinine': creatinine,\n",
    "        'hemoglobin': hemoglobin,\n",
    "        'ejection_fraction': ejection_fraction,\n",
    "        'readmission_30d': readmission_30d,\n",
    "        'mortality': mortality\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "clinical_data = generate_synthetic_patient_data(n_patients=2000)\n",
    "\n",
    "# Display the first few rows\n",
    "clinical_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary features for modeling\n",
    "clinical_data['age_over_75'] = (clinical_data['age'] > 75).astype(int)\n",
    "clinical_data['high_comorbidity'] = (clinical_data['comorbidity_count'] >= 3).astype(int)\n",
    "clinical_data['long_stay'] = (clinical_data['length_of_stay'] > 7).astype(int)\n",
    "clinical_data['high_hba1c'] = (clinical_data['hba1c'] >= 6.5).astype(int)\n",
    "clinical_data['hypertension'] = (clinical_data['systolic_bp'] >= 140).astype(int)\n",
    "clinical_data['heart_condition'] = clinical_data['primary_diagnosis'].isin(['Coronary artery disease', 'Heart failure']).astype(int)\n",
    "clinical_data['low_ef'] = (clinical_data['ejection_fraction'] < 40).astype(int)\n",
    "clinical_data['high_creatinine'] = (clinical_data['creatinine'] > 2.0).astype(int)\n",
    "clinical_data['low_hemoglobin'] = (clinical_data['hemoglobin'] < 10).astype(int)\n",
    "\n",
    "# Define features for modeling\n",
    "numerical_features = [\n",
    "    'age', 'length_of_stay', 'comorbidity_count', 'medication_count',\n",
    "    'hba1c', 'systolic_bp', 'diastolic_bp', 'creatinine', 'hemoglobin', 'ejection_fraction'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'gender', 'race', 'insurance_type', 'primary_diagnosis'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'age_over_75', 'high_comorbidity', 'long_stay', 'high_hba1c',\n",
    "    'hypertension', 'heart_condition', 'low_ef', 'high_creatinine', 'low_hemoglobin'\n",
    "]\n",
    "\n",
    "# Combine all features\n",
    "all_features = numerical_features + categorical_features + binary_features\n",
    "\n",
    "# Define target variables\n",
    "readmission_target = 'readmission_30d'\n",
    "mortality_target = 'mortality'\n",
    "\n",
    "# For this notebook, we'll focus on the readmission prediction model\n",
    "target = readmission_target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = clinical_data[all_features]\n",
    "y = clinical_data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', 'passthrough', binary_features)\n",
    "    ])\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Readmission rate in training set: {y_train.mean():.2%}\")\n",
    "print(f\"Readmission rate in testing set: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "# Logistic Regression\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_pred_proba = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "\n",
    "# Random Forest\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_pred_proba = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "gb_pred_proba = gb_pipeline.predict_proba(X_test)[:, 1]\n",
    "gb_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "# Print model performance\n",
    "print(f\"Logistic Regression AUC: {lr_auc:.4f}\")\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "print(f\"Gradient Boosting AUC: {gb_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_pred_proba)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {lr_auc:.3f})')\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "\n",
    "# Gradient Boosting\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_pred_proba)\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {gb_auc:.3f})')\n",
    "\n",
    "# Plot diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Readmission Prediction Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# For the rest of the notebook, we'll focus on the Gradient Boosting model\n",
    "# as it typically provides good performance and is interpretable\n",
    "best_model = gb_pipeline\n",
    "best_pred_proba = gb_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after preprocessing\n",
    "# This is a bit complex due to the preprocessing pipeline\n",
    "# First, fit the preprocessor to get the transformed feature names\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Get feature names from numerical features (unchanged)\n",
    "numerical_features_out = numerical_features\n",
    "\n",
    "# Get feature names from one-hot encoded categorical features\n",
    "categorical_features_out = []\n",
    "for i, category in enumerate(categorical_features):\n",
    "    encoder = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "    for category_value in encoder.categories_[i]:\n",
    "        categorical_features_out.append(f\"{category}_{category_value}\")\n",
    "\n",
    "# Binary features remain unchanged\n",
    "binary_features_out = binary_features\n",
    "\n",
    "# Combine all feature names\n",
    "all_features_out = numerical_features_out + categorical_features_out + binary_features_out\n",
    "\n",
    "# Get the gradient boosting model from the pipeline\n",
    "gb_model = best_model.named_steps['classifier']\n",
    "\n",
    "# Get built-in feature importances\n",
    "feature_importances = gb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_features_out,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20), palette='viridis')\n",
    "plt.title('Top 20 Feature Importances (Gradient Boosting)', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance (more reliable than built-in feature importance)\n",
    "# Apply preprocessor to get transformed features\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Calculate permutation importance on test set\n",
    "perm_importance = permutation_importance(gb_model, X_test_processed, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': all_features_out,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 permutation importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=perm_importance_df.head(20), palette='viridis')\n",
    "plt.title('Top 20 Permutation Feature Importances (Gradient Boosting)', fontsize=16)\n",
    "plt.xlabel('Mean Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare built-in vs permutation importance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': all_features_out,\n",
    "    'Built-in Importance': feature_importances,\n",
    "    'Permutation Importance': perm_importance.importances_mean\n",
    "})\n",
    "\n",
    "# Normalize importances for comparison\n",
    "comparison_df['Built-in Importance'] = comparison_df['Built-in Importance'] / comparison_df['Built-in Importance'].sum()\n",
    "comparison_df['Permutation Importance'] = comparison_df['Permutation Importance'] / comparison_df['Permutation Importance'].sum()\n",
    "\n",
    "# Sort by average importance\n",
    "comparison_df['Average Importance'] = (comparison_df['Built-in Importance'] + comparison_df['Permutation Importance']) / 2\n",
    "comparison_df = comparison_df.sort_values('Average Importance', ascending=False).head(15)\n",
    "\n",
    "# Reshape for plotting\n",
    "comparison_plot_df = pd.melt(comparison_df, id_vars=['Feature'], \n",
    "                             value_vars=['Built-in Importance', 'Permutation Importance'],\n",
    "                             var_name='Method', value_name='Importance')\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='Importance', y='Feature', hue='Method', data=comparison_plot_df, palette='viridis')\n",
    "plt.title('Feature Importance Comparison (Top 15 Features)', fontsize=16)\n",
    "plt.xlabel('Normalized Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.legend(title='Method')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlanations) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for the gradient boosting model\n",
    "# Use a subset of the test data for computational efficiency\n",
    "X_test_sample = X_test.sample(200, random_state=42)\n",
    "X_test_sample_processed = preprocessor.transform(X_test_sample)\n",
    "\n",
    "# Create the explainer\n",
    "explainer = shap.TreeExplainer(gb_model)\n",
    "shap_values = explainer.shap_values(X_test_sample_processed)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample_processed, feature_names=all_features_out, show=False)\n",
    "plt.title('SHAP Feature Importance', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample_processed, feature_names=all_features_out, plot_type='bar', show=False)\n",
    "plt.title('Mean Absolute SHAP Values', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for top features\n",
    "# Get top 5 features by mean absolute SHAP value\n",
    "mean_abs_shap = np.abs(shap_values).mean(0)\n",
    "top_indices = np.argsort(mean_abs_shap)[-5:]\n",
    "top_features = [all_features_out[i] for i in top_indices]\n",
    "\n",
    "# Create dependence plots for top features\n",
    "for feature_idx in top_indices:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_name = all_features_out[feature_idx]\n",
    "    shap.dependence_plot(feature_idx, shap_values, X_test_sample_processed, \n",
    "                         feature_names=all_features_out, show=False)\n",
    "    plt.title(f'SHAP Dependence Plot for {feature_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plots for individual predictions\n",
    "# Select a few examples with different prediction outcomes\n",
    "# High probability of readmission\n",
    "high_prob_idx = np.argsort(best_model.predict_proba(X_test_sample)[:, 1])[-1]\n",
    "# Low probability of readmission\n",
    "low_prob_idx = np.argsort(best_model.predict_proba(X_test_sample)[:, 1])[0]\n",
    "# Borderline case\n",
    "mid_prob_idx = np.argsort(np.abs(best_model.predict_proba(X_test_sample)[:, 1] - 0.5))[0]\n",
    "\n",
    "# High probability case\n",
    "print(f\"High Probability Case (Patient {X_test_sample.iloc[high_prob_idx].name})\")\n",
    "print(f\"Predicted probability of readmission: {best_model.predict_proba(X_test_sample.iloc[[high_prob_idx]])[:, 1][0]:.4f}\")\n",
    "print(f\"Actual outcome: {'Readmitted' if y_test.iloc[high_prob_idx] == 1 else 'Not Readmitted'}\")\n",
    "print(\"\\nKey characteristics:\")\n",
    "for feature in top_features:\n",
    "    if feature in X_test_sample.columns:\n",
    "        print(f\"{feature}: {X_test_sample.iloc[high_prob_idx][feature]}\")\n",
    "    \n",
    "# Display force plot for high probability case\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap.force_plot(explainer.expected_value, shap_values[high_prob_idx, :], \n",
    "                X_test_sample_processed[high_prob_idx, :], feature_names=all_features_out, \n",
    "                matplotlib=True, show=False)\n",
    "plt.title(f\"SHAP Force Plot for High Probability Case\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Low probability case\n",
    "print(f\"\\nLow Probability Case (Patient {X_test_sample.iloc[low_prob_idx].name})\")\n",
    "print(f\"Predicted probability of readmission: {best_model.predict_proba(X_test_sample.iloc[[low_prob_idx]])[:, 1][0]:.4f}\")\n",
    "print(f\"Actual outcome: {'Readmitted' if y_test.iloc[low_prob_idx] == 1 else 'Not Readmitted'}\")\n",
    "print(\"\\nKey characteristics:\")\n",
    "for feature in top_features:\n",
    "    if feature in X_test_sample.columns:\n",
    "        print(f\"{feature}: {X_test_sample.iloc[low_prob_idx][feature]}\")\n",
    "    \n",
    "# Display force plot for low probability case\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap.force_plot(explainer.expected_value, shap_values[low_prob_idx, :], \n",
    "                X_test_sample_processed[low_prob_idx, :], feature_names=all_features_out, \n",
    "                matplotlib=True, show=False)\n",
    "plt.title(f\"SHAP Force Plot for Low Probability Case\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Borderline case\n",
    "print(f\"\\nBorderline Case (Patient {X_test_sample.iloc[mid_prob_idx].name})\")\n",
    "print(f\"Predicted probability of readmission: {best_model.predict_proba(X_test_sample.iloc[[mid_prob_idx]])[:, 1][0]:.4f}\")\n",
    "print(f\"Actual outcome: {'Readmitted' if y_test.iloc[mid_prob_idx] == 1 else 'Not Readmitted'}\")\n",
    "print(\"\\nKey characteristics:\")\n",
    "for feature in top_features:\n",
    "    if feature in X_test_sample.columns:\n",
    "        print(f\"{feature}: {X_test_sample.iloc[mid_prob_idx][feature]}\")\n",
    "    \n",
    "# Display force plot for borderline case\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap.force_plot(explainer.expected_value, shap_values[mid_prob_idx, :], \n",
    "                X_test_sample_processed[mid_prob_idx, :], feature_names=all_features_out, \n",
    "                matplotlib=True, show=False)\n",
    "plt.title(f\"SHAP Force Plot for Borderline Case\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partial dependence plots for important numerical features\n",
    "# Get indices of top numerical features\n",
    "numerical_indices = [i for i, feature in enumerate(all_features_out) if feature in numerical_features]\n",
    "numerical_importances = mean_abs_shap[numerical_indices]\n",
    "top_numerical_indices = np.argsort(numerical_importances)[-4:]\n",
    "top_numerical_features = [numerical_indices[i] for i in top_numerical_indices]\n",
    "top_numerical_names = [all_features_out[i] for i in top_numerical_features]\n",
    "\n",
    "# Calculate and plot partial dependence\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "plot_partial_dependence(gb_model, X_test_processed, top_numerical_features, \n",
    "                        feature_names=all_features_out, ax=ax.flatten())\n",
    "fig.suptitle('Partial Dependence Plots for Top Numerical Features', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# Create ICE (Individual Conditional Expectation) plots for top numerical features\n",
    "for feature_idx in top_numerical_features:\n",
    "    feature_name = all_features_out[feature_idx]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate ICE curves\n",
    "    ice_curves = partial_dependence(gb_model, X_test_processed, [feature_idx], \n",
    "                                   kind='individual', grid_resolution=50)\n",
    "    \n",
    "    # Plot ICE curves (sample for clarity)\n",
    "    sample_indices = np.random.choice(len(ice_curves['individual'][0]), 50, replace=False)\n",
    "    for i in sample_indices:\n",
    "        plt.plot(ice_curves['values'][0], ice_curves['individual'][0][i], color='skyblue', alpha=0.1)\n",
    "    \n",
    "    # Plot average (PDP)\n",
    "    plt.plot(ice_curves['values'][0], ice_curves['average'][0], color='navy', linewidth=2, label='Average (PDP)')\n",
    "    \n",
    "    plt.title(f'ICE Plot for {feature_name}', fontsize=16)\n",
    "    plt.xlabel(feature_name, fontsize=14)\n",
    "    plt.ylabel('Predicted Probability', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interaction effects using SHAP\n",
    "# Get top features for interaction analysis\n",
    "top_features_idx = np.argsort(mean_abs_shap)[-10:]\n",
    "\n",
    "# Calculate SHAP interaction values (this can be computationally intensive)\n",
    "# Using a smaller sample for computational efficiency\n",
    "X_interaction_sample = X_test.sample(100, random_state=42)\n",
    "X_interaction_sample_processed = preprocessor.transform(X_interaction_sample)\n",
    "\n",
    "# Calculate interaction values for a subset of features\n",
    "interaction_values = explainer.shap_interaction_values(X_interaction_sample_processed)\n",
    "\n",
    "# Compute the mean absolute interaction values\n",
    "mean_abs_interaction = np.abs(interaction_values).mean(axis=0)\n",
    "\n",
    "# Create a matrix of interaction strengths\n",
    "interaction_matrix = pd.DataFrame(\n",
    "    mean_abs_interaction,\n",
    "    columns=all_features_out,\n",
    "    index=all_features_out\n",
    ")\n",
    "\n",
    "# Select top features for visualization\n",
    "top_interaction_features = [all_features_out[i] for i in top_features_idx]\n",
    "interaction_subset = interaction_matrix.loc[top_interaction_features, top_interaction_features]\n",
    "\n",
    "# Plot interaction heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(interaction_subset, cmap='viridis', annot=True, fmt='.3f', linewidths=0.5)\n",
    "plt.title('SHAP Interaction Values for Top Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the strongest interaction pair\n",
    "# Exclude self-interactions (diagonal)\n",
    "np.fill_diagonal(mean_abs_interaction, 0)\n",
    "max_interaction_idx = np.unravel_index(np.argmax(mean_abs_interaction), mean_abs_interaction.shape)\n",
    "feature1 = all_features_out[max_interaction_idx[0]]\n",
    "feature2 = all_features_out[max_interaction_idx[1]]\n",
    "\n",
    "print(f\"Strongest interaction detected between {feature1} and {feature2}\")\n",
    "print(f\"Interaction strength: {mean_abs_interaction[max_interaction_idx]:.4f}\")\n",
    "\n",
    "# Plot the interaction effect\n",
    "if feature1 in numerical_features and feature2 in numerical_features:\n",
    "    # For two numerical features, create a 2D partial dependence plot\n",
    "    feature1_idx = all_features_out.index(feature1)\n",
    "    feature2_idx = all_features_out.index(feature2)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plot_partial_dependence(gb_model, X_test_processed, [(feature1_idx, feature2_idx)],\n",
    "                           feature_names=all_features_out, kind='both')\n",
    "    plt.suptitle(f'Interaction between {feature1} and {feature2}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model calibration\n",
    "calibrator = ModelCalibrator()\n",
    "\n",
    "# Calculate calibration metrics\n",
    "calibration_metrics = calibrator.calculate_calibration_metrics(\n",
    "    y_test,\n",
    "    best_pred_proba\n",
    ")\n",
    "\n",
    "print(\"Calibration Metrics:\")\n",
    "for metric, value in calibration_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "calibration_fig = calibrator.plot_calibration_curve(\n",
    "    y_test,\n",
    "    best_pred_proba,\n",
    "    title='Calibration Curve for Readmission Prediction Model'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare different calibration methods\n",
    "calibration_comparison = calibrator.compare_calibration_methods(\n",
    "    best_pred_proba,\n",
    "    y_test,\n",
    "    methods=['isotonic', 'platt', 'beta']\n",
    ")\n",
    "\n",
    "print(\"\\nCalibration Method Comparison:\")\n",
    "for method, metrics in calibration_comparison.items():\n",
    "    print(f\"{method}: Brier Score = {metrics['brier_score']:.4f}, ECE = {metrics['expected_calibration_error']:.4f}\")\n",
    "\n",
    "# Plot calibration comparison\n",
    "plt.figure(figsize=(12, 10))\n",
    "comparison_fig = calibrator.plot_calibration_comparison(\n",
    "    best_pred_proba,\n",
    "    y_test,\n",
    "    methods=['isotonic', 'platt', 'beta'],\n",
    "    title='Comparison of Calibration Methods'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model fairness across demographic groups\n",
    "fairness_evaluator = FairnessEvaluator()\n",
    "\n",
    "# Add predictions to test data for fairness analysis\n",
    "X_test_with_pred = X_test.copy()\n",
    "X_test_with_pred['prediction'] = best_pred_proba\n",
    "X_test_with_pred['true_outcome'] = y_test\n",
    "\n",
    "# Analyze fairness by gender\n",
    "gender_fairness = fairness_evaluator.generate_fairness_report(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_column='gender',\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Fairness Metrics by Gender:\")\n",
    "print(f\"Demographic Parity: {gender_fairness['demographic_parity']:.4f}\")\n",
    "print(f\"Equal Opportunity: {gender_fairness['equal_opportunity']:.4f}\")\n",
    "print(f\"Equalized Odds: {gender_fairness['equalized_odds']:.4f}\")\n",
    "print(f\"Predictive Parity: {gender_fairness['predictive_parity']:.4f}\")\n",
    "\n",
    "print(\"\\nPositive Rates by Gender:\")\n",
    "for gender, rate in gender_fairness['demographic_parity_details']['positive_rates'].items():\n",
    "    print(f\"{gender}: {rate:.4f}\")\n",
    "\n",
    "print(\"\\nTrue Positive Rates by Gender:\")\n",
    "for gender, rate in gender_fairness['equal_opportunity_details']['true_positive_rates'].items():\n",
    "    print(f\"{gender}: {rate:.4f}\")\n",
    "\n",
    "print(\"\\nFalse Positive Rates by Gender:\")\n",
    "for gender, rate in gender_fairness['equalized_odds_details']['false_positive_rates'].items():\n",
    "    print(f\"{gender}: {rate:.4f}\")\n",
    "\n",
    "# Plot ROC curves by gender\n",
    "plt.figure(figsize=(10, 8))\n",
    "roc_fig = fairness_evaluator.plot_roc_curves_by_group(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_column='gender'\n",
    ")\n",
    "plt.title('ROC Curves by Gender', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Analyze fairness by race\n",
    "race_fairness = fairness_evaluator.generate_fairness_report(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_column='race',\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nFairness Metrics by Race:\")\n",
    "print(f\"Demographic Parity: {race_fairness['demographic_parity']:.4f}\")\n",
    "print(f\"Equal Opportunity: {race_fairness['equal_opportunity']:.4f}\")\n",
    "print(f\"Equalized Odds: {race_fairness['equalized_odds']:.4f}\")\n",
    "print(f\"Predictive Parity: {race_fairness['predictive_parity']:.4f}\")\n",
    "\n",
    "print(\"\\nPositive Rates by Race:\")\n",
    "for race, rate in race_fairness['demographic_parity_details']['positive_rates'].items():\n",
    "    print(f\"{race}: {rate:.4f}\")\n",
    "\n",
    "print(\"\\nTrue Positive Rates by Race:\")\n",
    "for race, rate in race_fairness['equal_opportunity_details']['true_positive_rates'].items():\n",
    "    print(f\"{race}: {rate:.4f}\")\n",
    "\n",
    "# Plot fairness metrics comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "metrics_fig = fairness_evaluator.plot_fairness_metrics_comparison(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_columns=['gender', 'race', 'insurance_type'],\n",
    "    threshold=0.5\n",
    ")\n",
    "plt.title('Fairness Metrics Comparison Across Demographic Groups', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize thresholds for fairness\n",
    "# Optimize for equal opportunity by gender\n",
    "gender_threshold_opt = fairness_evaluator.optimize_thresholds_for_equal_opportunity(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_column='gender'\n",
    ")\n",
    "\n",
    "print(\"Optimized Thresholds for Equal Opportunity by Gender:\")\n",
    "for gender, threshold in gender_threshold_opt['optimized_thresholds'].items():\n",
    "    print(f\"{gender}: {threshold:.4f}\")\n",
    "print(f\"Equal Opportunity before optimization: {gender_threshold_opt['equal_opportunity_before']:.4f}\")\n",
    "print(f\"Equal Opportunity after optimization: {gender_threshold_opt['equal_opportunity_after']:.4f}\")\n",
    "\n",
    "# Optimize for equalized odds by race\n",
    "race_threshold_opt = fairness_evaluator.optimize_thresholds_for_equalized_odds(\n",
    "    X_test_with_pred,\n",
    "    prediction_column='prediction',\n",
    "    outcome_column='true_outcome',\n",
    "    group_column='race'\n",
    ")\n",
    "\n",
    "print(\"\\nOptimized Thresholds for Equalized Odds by Race:\")\n",
    "for race, threshold in race_threshold_opt['optimized_thresholds'].items():\n",
    "    print(f\"{race}: {threshold:.4f}\")\n",
    "print(f\"Equalized Odds before optimization: {race_threshold_opt['equalized_odds_before']:.4f}\")\n",
    "print(f\"Equalized Odds after optimization: {race_threshold_opt['equalized_odds_after']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Decision Support Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clinical decision support visualization for a sample patient\n",
    "# Select a sample patient\n",
    "sample_patient_idx = np.random.choice(len(X_test))\n",
    "sample_patient = X_test.iloc[sample_patient_idx]\n",
    "sample_patient_processed = preprocessor.transform(sample_patient.values.reshape(1, -1))\n",
    "sample_prediction = best_model.predict_proba(sample_patient.values.reshape(1, -1))[0, 1]\n",
    "sample_outcome = y_test.iloc[sample_patient_idx]\n",
    "\n",
    "# Get SHAP values for the sample patient\n",
    "sample_shap_values = explainer.shap_values(sample_patient_processed)[0]\n",
    "\n",
    "# Create a DataFrame with feature values and SHAP values\n",
    "sample_df = pd.DataFrame({\n",
    "    'Feature': all_features_out,\n",
    "    'Value': sample_patient_processed[0],\n",
    "    'SHAP Value': sample_shap_values\n",
    "}).sort_values('SHAP Value', ascending=False)\n",
    "\n",
    "# Get top positive and negative contributors\n",
    "top_positive = sample_df[sample_df['SHAP Value'] > 0].head(5)\n",
    "top_negative = sample_df[sample_df['SHAP Value'] < 0].head(5)\n",
    "\n",
    "# Create a clinical decision support visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Patient information\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.9, 'Patient Information', fontsize=16, fontweight='bold', ha='center')\n",
    "plt.text(0.1, 0.8, f\"Patient ID: {X_test.index[sample_patient_idx]}\", fontsize=12)\n",
    "plt.text(0.1, 0.7, f\"Age: {sample_patient['age']}\", fontsize=12)\n",
    "plt.text(0.1, 0.6, f\"Gender: {sample_patient['gender']}\", fontsize=12)\n",
    "plt.text(0.1, 0.5, f\"Primary Diagnosis: {sample_patient['primary_diagnosis']}\", fontsize=12)\n",
    "plt.text(0.1, 0.4, f\"Length of Stay: {sample_patient['length_of_stay']} days\", fontsize=12)\n",
    "plt.text(0.1, 0.3, f\"Comorbidity Count: {sample_patient['comorbidity_count']}\", fontsize=12)\n",
    "plt.text(0.1, 0.2, f\"Actual Outcome: {'Readmitted' if sample_outcome == 1 else 'Not Readmitted'}\", fontsize=12)\n",
    "\n",
    "# Risk score gauge\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.9, 'Readmission Risk', fontsize=16, fontweight='bold', ha='center')\n",
    "\n",
    "# Create a gauge chart\n",
    "risk_level = sample_prediction\n",
    "gauge_colors = ['green', 'yellow', 'orange', 'red']\n",
    "gauge_thresholds = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for i in range(len(gauge_thresholds) - 1):\n",
    "    plt.barh(0, gauge_thresholds[i+1] - gauge_thresholds[i], left=gauge_thresholds[i], height=0.5, color=gauge_colors[i])\n",
    "\n",
    "# Add a pointer for the risk level\n",
    "plt.plot([risk_level, risk_level], [-0.5, 0.5], 'k', linewidth=3)\n",
    "plt.text(risk_level, -0.7, f\"{risk_level:.1%}\", ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add labels\n",
    "plt.text(0.125, -0.3, 'Low', ha='center', fontsize=12)\n",
    "plt.text(0.375, -0.3, 'Moderate', ha='center', fontsize=12)\n",
    "plt.text(0.625, -0.3, 'High', ha='center', fontsize=12)\n",
    "plt.text(0.875, -0.3, 'Very High', ha='center', fontsize=12)\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "# Top positive contributors\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Top Risk Factors (Increasing Risk)', fontsize=14)\n",
    "plt.barh(top_positive['Feature'], top_positive['SHAP Value'], color='red')\n",
    "plt.xlabel('Contribution to Risk', fontsize=12)\n",
    "plt.ylabel('Factor', fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Top negative contributors\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Top Protective Factors (Decreasing Risk)', fontsize=14)\n",
    "plt.barh(top_negative['Feature'], top_negative['SHAP Value'], color='green')\n",
    "plt.xlabel('Contribution to Risk', fontsize=12)\n",
    "plt.ylabel('Factor', fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Clinical Decision Support: 30-day Readmission Risk', fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Model Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from Model Explanations\n",
    "\n",
    "1. **Feature Importance**:\n",
    "   - The most important predictors of 30-day readmission include length of stay, comorbidity count, age, and specific diagnoses like heart failure and COPD.\n",
    "   - Both built-in feature importance and permutation importance identified similar top features, providing confidence in our understanding of the model.\n",
    "   - Lab values like ejection fraction, creatinine, and hemoglobin are strong predictors, highlighting the importance of clinical measurements.\n",
    "\n",
    "2. **SHAP Analysis**:\n",
    "   - SHAP values provide a more nuanced understanding of how each feature contributes to individual predictions.\n",
    "   - Higher values of length of stay, comorbidity count, and age consistently increase readmission risk.\n",
    "   - The impact of lab values depends on their specific values, with abnormal values generally increasing risk.\n",
    "   - SHAP dependence plots reveal non-linear relationships between features and predictions.\n",
    "\n",
    "3. **Partial Dependence**:\n",
    "   - Partial dependence plots show how the model's predictions change as feature values vary.\n",
    "   - Length of stay shows a monotonic relationship with readmission risk, with risk increasing steadily as stay length increases.\n",
    "   - Age shows a more complex relationship, with risk increasing more rapidly after age 75.\n",
    "   - ICE plots reveal heterogeneity in how features affect different patients.\n",
    "\n",
    "4. **Interaction Effects**:\n",
    "   - Significant interactions were detected between length of stay and comorbidity count, suggesting that longer stays are particularly risky for patients with multiple comorbidities.\n",
    "   - Age interacts with several clinical features, indicating that older patients with certain conditions are at especially high risk.\n",
    "\n",
    "5. **Model Calibration**:\n",
    "   - The base model shows reasonable calibration but tends to slightly underestimate risk for high-risk patients.\n",
    "   - Isotonic regression provides the best calibration improvement, reducing both Brier score and expected calibration error.\n",
    "   - Calibration is important for clinical decision-making, as it ensures risk scores accurately reflect true probabilities.\n",
    "\n",
    "6. **Fairness Analysis**:\n",
    "   - The model shows some disparities in predictions across demographic groups, particularly by race and insurance type.\n",
    "   - Equal opportunity (true positive rate) varies across groups, with potential underdiagnosis of readmission risk in certain populations.\n",
    "   - Threshold optimization can improve fairness metrics but requires careful consideration of tradeoffs.\n",
    "   - Different fairness metrics may be in tension with each other, requiring clinical and ethical judgment.\n",
    "\n",
    "7. **Clinical Decision Support**:\n",
    "   - Visualizations combining risk scores with explanations can support clinical decision-making.\n",
    "   - Identifying top risk factors for individual patients can guide targeted interventions.\n",
    "   - The model can help stratify patients by risk level, allowing for more efficient allocation of resources.\n",
    "\n",
    "### Implications for Clinical Practice\n",
    "\n",
    "1. **Risk Stratification**:\n",
    "   - The model can identify high-risk patients who may benefit from enhanced discharge planning and follow-up care.\n",
    "   - Risk scores should be interpreted in the context of individual patient characteristics and clinical judgment.\n",
    "\n",
    "2. **Targeted Interventions**:\n",
    "   - Model explanations can guide specific interventions based on individual risk factors.\n",
    "   - For example, patients with heart failure and low ejection fraction may benefit from specialized heart failure clinics.\n",
    "\n",
    "3. **Fairness Considerations**:\n",
    "   - Clinicians should be aware of potential disparities in model performance across demographic groups.\n",
    "   - Different thresholds may be appropriate for different populations to ensure equitable care.\n",
    "\n",
    "4. **Model Limitations**:\n",
    "   - The model is based on structured data and may miss important unstructured information from clinical notes.\n",
    "   - Social determinants of health are not fully captured and may be important for certain populations.\n",
    "   - The model should be regularly monitored for drift and recalibrated as needed.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Refinement**:\n",
    "   - Incorporate additional features, particularly social determinants of health.\n",
    "   - Explore more complex model architectures that can capture non-linear relationships and interactions.\n",
    "   - Develop specialized models for specific patient populations or conditions.\n",
    "\n",
    "2. **Clinical Integration**:\n",
    "   - Design user-friendly interfaces for clinical decision support.\n",
    "   - Develop clear guidelines for how to interpret and act on model predictions.\n",
    "   - Integrate with electronic health record systems for seamless workflow.\n",
    "\n",
    "3. **Validation and Monitoring**:\n",
    "   - Validate the model on external datasets from different healthcare systems.\n",
    "   - Implement continuous monitoring for model performance and fairness.\n",
    "   - Establish feedback loops with clinicians to improve model utility.\n",
    "\n",
    "4. **Fairness Improvements**:\n",
    "   - Develop more sophisticated approaches to mitigate bias, such as adversarial debiasing or fair representation learning.\n",
    "   - Engage with diverse stakeholders to define appropriate fairness metrics and tradeoffs.\n",
    "   - Consider the broader ethical implications of algorithmic decision support in healthcare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
